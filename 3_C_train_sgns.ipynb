{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[과제9] 3_C_train_sgns.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"d6JSAWZsafJb"},"source":["# 3_C_train_sgns\n","- author: Eu-Bin KIM\n","- 20th of August 2021\n","- tlrndk123@gmail.com\n","\n","## 목표\n","- `SGNS.forward()` 구현\n","- `SGNS.training_step()` 구현\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3LNw_LHbu_F","executionInfo":{"status":"ok","timestamp":1629800584190,"user_tz":-540,"elapsed":4327,"user":{"displayName":"김영현","photoUrl":"","userId":"01862996594103916968"}},"outputId":"5851d3c3-1f03-4591-93c4-d3892431f182"},"source":["# pytorch 라이브러리 설치\n","# https://pytorch.org/docs/stable/index.html\n","!pip3 install torch  "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GVodfGJQbxna","executionInfo":{"status":"ok","timestamp":1629800592117,"user_tz":-540,"elapsed":6120,"user":{"displayName":"김영현","photoUrl":"","userId":"01862996594103916968"}}},"source":["import torch\n","from torch import nn  # 하나의 신경망을 구현하기 위해.\n","from torch.nn import functional as F  # 여러 로스함수 등을 사용하기 위해.\n","from torch.nn import init  # 랜덤하게 가중치를 설정하기 위해.\n","from typing import Tuple, List, Dict \n","from keras_preprocessing.text import Tokenizer\n","from torch.nn import functional as F  # 여러 로스함수 등을 사용하기 위해.\n","from torch.utils.data import Dataset, DataLoader  # 데이터 로더와 함께 쓰기 위해.\n","import torch  # 각종 타이핑을 위해\n","from tensorflow.keras.preprocessing.sequence import skipgrams\n","import torch.optim as optim\n","import logging\n","from sys import stdout\n","logging.basicConfig(stream=stdout, level=logging.INFO)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDZzZxfLZ88A","executionInfo":{"status":"ok","timestamp":1629800683105,"user_tz":-540,"elapsed":211,"user":{"displayName":"김영현","photoUrl":"","userId":"01862996594103916968"}}},"source":["# 이번숙제의 목표는, pytorch framework를 활용하여 SGNS를 직접 구현해보는 것.\n","# Negative Sampling 로직은 제가 직접 구현합니다. 여러분은 이 클래스만 구현하면 됩니다!\n","class SGNS(nn.Module):\n","    \"\"\"\n","    A pytorch implementation of SkipGram with Negative Sampling (SGNS).\n","    \"\"\"\n","    def __init__(self, vocab_size: int, embed_size: int, word2idx: Dict[str, int]):\n","        super(SGNS, self).__init__()\n","        self.vocab_size = vocab_size  # 어휘의 크기.\n","        self.embed_size = embed_size  # 임베딩 벡터의 크기.\n","        self.input_embeddings = nn.Embedding(vocab_size, embed_size)\n","        self.context_embeddings = nn.Embedding(vocab_size, embed_size)\n","        self.word2idx = word2idx\n","        # 각 임베딩 테이블을 랜덤 초기화 해준다.\n","        init.uniform_(self.input_embeddings.weight.data)\n","        init.constant_(self.context_embeddings.weight.data, 0)\n","\n","    def forward(self, X_input: torch.Tensor, X_context: torch.Tensor):\n","        \"\"\"\n","        :param X_input: (N,). 정수인코딩된 입력 단어들 (고유 아이디가 담긴 리스트).\n","        :param X_context: (N,). 정수인코딩된 맥락 단어들 (고유 아이디가 담긴 리스트).\n","        :return: 입력과 맥락의 유사도 벡터 (N,) (범위 = [0, 1])\n","        \"\"\"\n","        # --- TODO 1 --- #\n","        # 대문자로 시작하는 변수: 행렬.\n","        # 소문자로 시작하는 변수: 벡터.\n","        # 먼저, 입력과 맥락에 해당하는 모든 단어의 임베딩 벡터를 불러온다.\n","        Embed_input = self.input_embeddings(X_input) # (N,) -> (N, embed_size)\n","        Embed_context = self.context_embeddings(X_context) # (N,) -> (N, embed_size)\n","        # 입력 임베딩, 맥락 임베딩 사이의 유사도 점수(벡터의 내적)를 구해준다.\n","        sims = torch.sum(torch.mul(Embed_input, Embed_context), dim=1) # (N, embed_size), (N, embed_size) -> (N, )\n","        # sims 의 치역은 모든 실수이므로, [0, 1] 사이에 값이 놓이도록 시그모이드 함수로 값을 정규화 해준다.\n","        sims_normalized = F.sigmoid(sims) # (N, ) -> [0, 1]로 정규화 -> (N, )\n","        return sims_normalized\n","        # ------------- #\n","\n","    def training_step(self,\n","                      X_input: torch.Tensor,\n","                      X_context: torch.Tensor,\n","                      y: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param X_input (N, ) 정수인코딩된 입력 단어들 (고유 아이디가 담긴 리스트).\n","        :param X_context (N, ) 정수인코딩된 맥락 단어들 (고유 아이디가 담긴 리스트).\n","        :param y:  (N, ) 이웃=1/이웃x=0, 이진분류를 위한 레이블.\n","        :return: 해당 batch로부터 계산된 로스함수. (1,)\n","        \"\"\"\n","        # --- TODO 2 --- #\n","        sims_normalized = self.forward(X_input, X_context)  # (N,)\n","        loss = F.binary_cross_entropy(sims_normalized, y)  # criterion((N,), (N,)) -> (1,)\n","        return loss\n","        # -------------- #\n","    \n","    def cos_similarity(self, word_1: str, word_2: str) -> float:\n","      word_1_idx = self.word2idx[word_1]\n","      word_2_idx = self.word2idx[word_2]\n","      # just compute the.. sim matrix.d\n","      word_1_embed = self.input_embeddings(torch.LongTensor([word_1_idx]))\n","      word_2_embed = self.input_embeddings(torch.LongTensor([word_2_idx]))\n","      return torch.cosine_similarity(word_1_embed, word_2_embed).item()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"56wbmtTwUw0a","executionInfo":{"status":"ok","timestamp":1629800685900,"user_tz":-540,"elapsed":348,"user":{"displayName":"김영현","photoUrl":"","userId":"01862996594103916968"}}},"source":["# 말뭉치 구성 - 그냥 간단한... 데이터 구성.\n","CORPUS: List[str] = [\n","    \"They all admire her courage\",\n","    \"They all love her courage\",\n","    \"He hates his country's enemies\"\n","]\n","\n","\n","# 하이퍼 파라미터\n","WINDOW_SIZE = 3\n","EMBED_SIZE = 100\n","VOCAB_SIZE = 40\n","LR = 0.01\n","EPOCHS = 50\n","NEGATIVE_SAMPLES = 3\n","\n","\n","# --- negative samples를 구축하기 위한 함수 --- #\n","def build_rows(seqs: List[List[int]]) -> List[Tuple[Tuple[int, int], int]]:\n","    global VOCAB_SIZE, WINDOW_SIZE\n","    rows = list()\n","    for seq in seqs:\n","        skip_grams = skipgrams(sequence=seq,\n","                               vocabulary_size=VOCAB_SIZE,\n","                               window_size=WINDOW_SIZE,\n","                               negative_samples=NEGATIVE_SAMPLES)\n","        pairs, labels = skip_grams\n","        rows += [\n","            (tuple(pair), label)\n","            for pair, label in zip(pairs, labels)\n","        ]\n","    return rows\n","\n","\n","# --- builders --- #\n","def build_X_input(rows: List[Tuple[Tuple[int, int], int]]) -> torch.LongTensor:\n","    X_input = [\n","        row[0][0]\n","        for row in rows\n","    ]\n","    return torch.LongTensor(X_input)\n","\n","\n","def build_X_context(rows: List[Tuple[Tuple[int, int], int]]) -> torch.LongTensor:\n","    X_context = [\n","        row[0][1]\n","        for row in rows\n","    ]\n","    return torch.LongTensor(X_context)\n","\n","\n","def build_y(rows: List[Tuple[Tuple[int, int], int]]) -> torch.FloatTensor:\n","    y = [\n","        row[1]\n","        for row in rows\n","    ]\n","    return torch.FloatTensor(y)\n","\n","\n","# 데이터셋 구성\n","class SGNSDataset(Dataset):\n","    def __init__(self, seqs: List[List[int]]):\n","        rows = build_rows(seqs)\n","        self.X_input = build_X_input(rows)\n","        self.X_output = build_X_context(rows)\n","        self.y = build_y(rows)\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Returning the size of the dataset\n","        :return:\n","        \"\"\"\n","        return self.y.shape[0]\n","\n","    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.LongTensor]:\n","        \"\"\"\n","        Returns features & the label\n","        :param idx:\n","        :return:\n","        \"\"\"\n","        return self.X_input[idx], self.X_input[idx], self.y[idx]\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"NE5ZI34GUyZD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629800689958,"user_tz":-540,"elapsed":1538,"user":{"displayName":"김영현","photoUrl":"","userId":"01862996594103916968"}},"outputId":"44f13490-b4ae-4a15-d8a7-e6ecd16e2a45"},"source":["# 테스트용 코드 - 모두 이해할필요는 없습니다.\n","# 로스가 작아지는지만 확인할 수 있으면 됩니다.\n","# 토크나이즈 & 정수인코딩 된것들.\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(texts=CORPUS)\n","seqs = tokenizer.texts_to_sequences(texts=CORPUS)  \n","dataset = SGNSDataset(seqs)\n","sgns = SGNS(vocab_size=VOCAB_SIZE, embed_size=EMBED_SIZE, word2idx=tokenizer.word_index)\n","optimizer = optim.Adam(params=sgns.parameters(), lr=LR)\n","dataloader = DataLoader(dataset, batch_size=10)\n","for e_idx, epoch in enumerate(range(EPOCHS)):\n","    losses = list() # 로스 저장\n","    for b_idx, batch in enumerate(dataloader):\n","        X_input, X_context, y = batch\n","        loss = sgns.training_step(X_input, X_context, y)\n","        optimizer.zero_grad()  # resetting the gradients\n","        loss.backward()  # backpropagation\n","        optimizer.step()  # gradient step\n","        # 각 배치의 로스를 저장.\n","        losses.append(loss.item())\n","    # 에폭이 끝날 때마다 평균 로스를 출력. = 데이터를 우려먹는 횟수.\n","    avg_loss = (sum(losses) / len(losses))\n","    print(\"{}: {}\".format(e_idx, avg_loss))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 0.6360471004789526\n","1: 0.624479849907485\n","2: 0.6222527758641676\n","3: 0.6199913891878995\n","4: 0.5971720584414222\n","5: 0.5904165601188486\n","6: 0.5881355066191066\n","7: 0.5848686668005857\n","8: 0.5835355967283249\n","9: 0.5826276892965491\n","10: 0.5816626833243803\n","11: 0.5814870243722742\n","12: 0.5807738832452081\n","13: 0.5808008191260424\n","14: 0.5801790004426782\n","15: 0.5801715783097527\n","16: 0.5796865400942889\n","17: 0.5796423757618124\n","18: 0.579289747910066\n","19: 0.5792019028555263\n","20: 0.5788792141459205\n","21: 0.5787180242213336\n","22: 0.5784347978505221\n","23: 0.578304414044727\n","24: 0.5780907056548379\n","25: 0.5779733861034567\n","26: 0.577828284014355\n","27: 0.577692061662674\n","28: 0.5774722383780913\n","29: 0.5773446600545536\n","30: 0.5771903490478342\n","31: 0.5770576623353091\n","32: 0.5769497657364066\n","33: 0.5768876604058526\n","34: 0.5767957608808171\n","35: 0.5766107087785547\n","36: 0.5765078270977194\n","37: 0.5764253288507462\n","38: 0.5762275403196161\n","39: 0.5761238038539886\n","40: 0.5760240771553733\n","41: 0.5759555250406265\n","42: 0.5760092572732405\n","43: 0.5759340890429236\n","44: 0.5756992453878577\n","45: 0.5755932778120041\n","46: 0.5755461752414703\n","47: 0.575523778796196\n","48: 0.5753822326660156\n","49: 0.5752849348566749\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CQa3nFgyZE2T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629800692720,"user_tz":-540,"elapsed":218,"user":{"displayName":"김영현","photoUrl":"","userId":"01862996594103916968"}},"outputId":"f613bb6d-7900-48a1-ba96-b0c8880469f6"},"source":["# hates 보다 loves 사이의 유사도 높으면 성공입니다!\n","print(sgns.cos_similarity(\"admire\", \"love\"))\n","print(sgns.cos_similarity(\"admire\", \"hates\"))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.6550629138946533\n","0.5786426067352295\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b3YKJvoqJuKK"},"source":[""],"execution_count":null,"outputs":[]}]}