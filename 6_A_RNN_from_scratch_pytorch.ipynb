{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6_A_RNN_from_scratch_pytorch.ipynb","provenance":[],"authorship_tag":"ABX9TyN1caVSwZ0Aj0Cs+HC+ZB2f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"eoIBzhQaFkQs"},"source":["\"\"\"\n","간단한 감성분석을 할 수 있는 간단한 RNN을 pytorch로 개발하는 것이 목표.\n","\"\"\"\n","\n","import torch\n","from typing import List, Tuple\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from keras_preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","\n","\n","# 신경망을 만들고 싶다 -> torch.nn.Module을 상속받기.\n","class SimpleRNN(torch.nn.Module):\n","    def __init__(self, vocab_size: int, hidden_size: int, embed_size: int):\n","        \"\"\"\n","        :param vocab_size: 말뭉치 속 고유한 단어의 개수. |V|\n","        :param hidden_size: rnn의 hidden vector의 차원의 크기. H\n","        \"\"\"\n","        # 학습하고자하는 가중치를 정의해주면 됩니다.\n","        super().__init__()\n","        # 정수인코딩된 나열을 그대로 입력으로 넣으면, 글자의 크기를 학습하게된다.\n","        # 그래서 원핫인코딩 or 임베딩.\n","        # Embedding layer = (|V|, E)\n","        self.hidden_size = hidden_size\n","        self.Embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n","        # h_t = f_W(h_t-1, x_t)\n","        # 정무님 - Wa, Wy, ba, by\n","        # 우리가 배웠던 기호 - W_hh, b_hh, Wxh, b_hh\n","        # h_t = tanh(W_hh * h_t-1 + W_xh * x_t)\n","        # h_t-1 (N, H) 과 h_t (N, H) 의 차원은 동일하다.\n","        # 즉. h_t-1 * W_hh  =  (N, H) * (H, H) = (N, H)\n","        # (N, L)\n","        # -> embeding layer -> (N, E) = x_t\n","        # x_t * W_xh = (N, E) * (E, H) =  (N, H)\n","        self.W_hh = torch.nn.Linear(in_features=hidden_size, out_features=hidden_size)  # (H, H)\n","        self.W_xh = torch.nn.Linear(in_features=embed_size, out_features=hidden_size)  # (E, H)\n","        self.W_hy = torch.nn.Linear(in_features=hidden_size, out_features=1)  # (H, 1)\n","\n","    def forward(self, X: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param X: 정수 인코딩된 나열의 배치 [[1, 2, 3, 0, 0], [5, 3, 2, 1, 0]]  (N, L)\n","        :return: 마지막 시간대의 hidden state를 출력 (N, H)\n","        \"\"\"\n","        # forward pass를 했을때 나오는 출력값을 계산\n","        # h_t = tanh(W_hh * h_t-1 + W_xh * x_t)\n","        # 첫번째 h_t는 무의미한 값이 들어간다.\n","        # H_0 = torch.zeros(size=(X.shape[0], self.hidden_size))\n","        # # 그 다음으로는 뭘 해야돼죠?\n","        # # 지금 H_0을 구했다.\n","        # # H_1 = tanh(H_0 * W_hh + X_1 * W_xh)\n","        # H_1 = torch.tanh(H_0 * self.W_hh + X[:, 0] * self.W_xh)\n","        # H_2 = torch.tanh(H_1 * self.W_hh + X[:, 1] * self.W_xh)\n","        # H_3 = torch.tanh(H_2 * self.W_hh + X[:, 2] * self.W_xh)\n","        # 그런데 이렇게 계속 반복을 하는 것보단, 일반화를 하는 것이 낫지 않을까?\n","        # # for loop\n","        H_t = torch.zeros(size=(X.shape[0], self.hidden_size))\n","        for t in range(X.shape[1]):\n","            Embed_t = self.Embedding(X[:, t])\n","            H_t = torch.tanh(self.W_hh(H_t) + self.W_xh(Embed_t))\n","        # many to one (sentiment analysis)\n","        return H_t\n","\n","    def training_step(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param X: 정수 인코딩된 나열의 배치 [[1, 2, 3, 0, 0], [5, 3, 2, 1, 0]]  (N, L)\n","        :param y: 긍/부정 레이블 [0, 1, 0, 0 , 1]\n","        :return: loss (1)\n","        \"\"\"\n","        # 배치가 들어왔을 때, 로스를 계산하면 된다.\n","        H_last = self.forward(X)  # (N, L) -> (N, H)\n","        # 이것으로부터, 긍정 / 부정일 확률을 구하고 싶다.\n","        # (N, H) * ???-(H, 1) -> (N, 1) - 어떻게 할 수 있을까?\n","        Hy = self.W_hy(H_last)  # (N, H) * (H, 1) -> (N, 1)\n","        # 그럼 Hy 의 범위가  [0, 1]?  아니다.\n","        # (N, 1)\n","        Hy_normalized = torch.sigmoid(Hy)  # [-inf, inf] -> [0, 1]\n","        # (N, 1), (N,) -> 하나로 통일하기 위해, y의 맞춘다.\n","        Hy_normalized = torch.reshape(Hy_normalized, y.shape)\n","        # N개의 로스를 다 더해서, 이 배치의 로스를 구한다.\n","        loss = F.binary_cross_entropy(input=Hy_normalized, target=y).sum()\n","        return loss\n","\n","# 어떤 데이터로 학습할까?\n","# 10개만 받겠습니다.\n","# 긍정적인 문장 5개를 적어주세요.\n","DATA: List[Tuple[str, int]] = [\n","    # 병국님\n","    (\"오늘도 수고했어\", 1),\n","    # 영성님\n","    (\"너는 할 수 있어\", 1),\n","    # 정무님\n","    (\"오늘 내 주식이 올랐다\", 1),\n","    # 우철님\n","    (\"오늘 날씨가 좋다\", 1),\n","    # 유빈님\n","    (\"난 너를 좋아해\", 1),\n","    # --- 부정적인 문장 - 레이블 = 0\n","    (\"난 너를 싫어해\", 0),\n","    # 병국님\n","    (\"넌 잘하는게 뭐냐?\", 0),\n","    # 선희님\n","    (\"너 때문에 다 망쳤어\", 0),\n","    # 정무님\n","    (\"오늘 피곤하다\", 0),\n","    # 유빈님\n","    (\"난 삼성을 싫어해\", 0)\n","]\n","\n","\n","# builders - 데이터를 텐서로 변환해주는 함수.\n","def build_X(sents: List[str], tokenizer: Tokenizer) -> torch.Tensor:\n","    tokenizer.fit_on_texts(sents)\n","    seqs = tokenizer.texts_to_sequences(sents)\n","    # 병국님, 영성님 - 패딩처리가 필요하다. 문장의 길이가 다 다르므로, 규격의 통일이 필요하다.\n","    seqs = pad_sequences(sequences=seqs, padding='post', value=0)\n","    return torch.LongTensor(seqs)\n","\n","\n","def build_y(labels: List[int]) -> torch.Tensor:\n","    return torch.FloatTensor(labels)\n","\n","\n","# 파이토치에서는 데이터셋을 정의해준느 것이 편하다.\n","class SimpleDataset(Dataset):\n","    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n","        self.X = X\n","        self.y = y\n","\n","    # 첫번째 - 데이터 셋의 크기를 알려주는 함수.\n","    def __len__(self) -> int:\n","        return self.y.shape[0]\n","\n","    def __getitem__(self, item: int) -> Tuple[torch.Tensor, torch.Tensor]:\n","        return self.X[item], self.y[item]\n","\n","\n","class SentimentAnalyzer:\n","    def __init__(self, rnn: SimpleRNN, tokenizer: Tokenizer):\n","        self.rnn = rnn\n","        self.tokenizer = tokenizer\n","\n","    def __call__(self, text: str) -> float:\n","        X = build_X(sents=[text], tokenizer=self.tokenizer)\n","        Ht = self.rnn.forward(X)\n","        Hy = self.rnn.W_hy(Ht)\n","        Hy_normalized = torch.sigmoid(Hy)\n","        return Hy_normalized.item()\n","\n","\n","def main():\n","    # 문장만,\n","    sents = [\n","        sent\n","        for sent, _ in DATA\n","    ]\n","    # 레이블만,\n","    labels = [\n","        label\n","        for _, label in DATA\n","    ]\n","    # 정무님: sents: List[str] -> List[int]\n","    tokenizer = Tokenizer(char_level=True)\n","    tokenizer.fit_on_texts(sents)\n","\n","    # 데이터 구축\n","    X = build_X(sents, tokenizer)\n","    y = build_y(labels)\n","    dataset = SimpleDataset(X, y)  # 파이토치 데이터 셋에 텐서를 저장.\n","    dataloader = DataLoader(dataset=dataset, batch_size=3, shuffle=True)\n","\n","    # 모델 만들기\n","    vocab_size = len(tokenizer.word_index) # 정수 1부터 부여.\n","    vocab_size += 1  # 왜 하나 더 늘려야할까? - padding token 0.\n","    rnn = SimpleRNN(vocab_size=vocab_size, hidden_size=16, embed_size=8)\n","\n","    # 어떤 최적화 알고리즘을 쓸까?\n","    optimizer = torch.optim.Adam(params=rnn.parameters(), lr=0.001)\n","\n","    # 에폭 루프\n","    for e_idx in range(300):\n","        # 배치루프\n","        losses = list()\n","        for b_idx, batch in enumerate(dataloader):\n","            X, y = batch\n","            # 영성님 - 학습을 해주어야 한다.\n","            # 그럼 무엇을 계산해야하는가\n","            loss: torch.Tensor = rnn.training_step(X, y)\n","            optimizer.zero_grad()\n","            loss.backward()  # 오차역전파 (모든 기울기를 다 계산)\n","            # 가중치를 경사도 하강법을 업데이트\n","            optimizer.step()\n","            # 기울기가 축적이 되는 것을 방지하기 위해 리셋\n","            losses.append(loss.item())\n","        avg_loss = (sum(losses) / len(losses))\n","        print(\"epoch:{}, avg_loss:{}\".format(e_idx, avg_loss))\n","\n","    analyser = SentimentAnalyzer(rnn, tokenizer)\n","    print(\"####-------- inferece ---- -#####\")\n","    print(analyser(text=\"오늘 날씨가 좋다\")) # 0.93의 확률로 출력!\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]}]}