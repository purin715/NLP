{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6_B_lstm.ipynb","provenance":[],"authorship_tag":"ABX9TyPOrQVee7lVcMsXt+tK/ofJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"GiSqdyrLFtoz"},"source":["\"\"\"\n","LSTM을 파이토치로 구현해보기!\n","\"\"\"\n","\n","from typing import List, Tuple\n","from keras_preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","import torch\n","from torch.nn import functional as F\n","\n","DATA: List[Tuple[str, int]] = [\n","    # 긍정적인 문장 - 1\n","    (\"나는 자연어처리\", 1),\n","    (\"도움이 되었으면\", 1),\n","    # 병국님\n","    (\"오늘도 수고했어\", 1),\n","    # 영성님\n","    (\"너는 할 수 있어\", 1),\n","    # 정무님\n","    (\"오늘 내 주식이 올랐다\", 1),\n","    # 우철님\n","    (\"오늘 날씨가 좋다\", 1),\n","    # 유빈님\n","    (\"난 너를 좋아해\", 1),\n","    # 다운님\n","    (\"지금 정말 잘하고 있어\", 1),\n","    # 민종님\n","    (\"지금처럼만 하면 잘될거야\", 1),\n","    (\"사랑해\", 1),\n","    (\"저희 허락없이 아프지 마세요\", 1),\n","    (\"오늘 점심 맛있다\", 1),\n","    (\"오늘 너무 예쁘다\", 1),\n","    # 다운님\n","    (\"곧 주말이야\", 1),\n","    # 재용님\n","    (\"오늘 주식이 올랐어\", 1),\n","    # 병운님\n","    (\"우리에게 빛나는 미래가 있어\", 1),\n","    # 재용님\n","    (\"너는 참 잘생겼어\", 1),\n","    # 윤서님\n","    (\"콩나물 무침은 맛있어\", 1),\n","    # 정원님\n","    (\"강사님 보고 싶어요\", 1),\n","    # 정원님\n","    (\"오늘 참 멋있었어\", 1),\n","    # 예은님\n","    (\"맛있는게 먹고싶다\", 1),\n","    # 민성님\n","    (\"로또 당첨됐어\", 1),\n","    # 민성님\n","    (\"이 음식은 맛이 없을수가 없어\", 1),\n","    # 경서님\n","    (\"오늘도 좋은 하루보내요\", 1),\n","    # 성민님\n","    (\"내일 시험 안 본대\", 1),\n","    # --- 부정적인 문장 - 레이블 = 0\n","    (\"난 너를 싫어해\", 0),\n","    # 병국님\n","    (\"넌 잘하는게 뭐냐?\", 0),\n","    # 선희님\n","    (\"너 때문에 다 망쳤어\", 0),\n","    # 정무님\n","    (\"오늘 피곤하다\", 0),\n","    # 유빈님\n","    (\"난 삼성을 싫어해\", 0),\n","    (\"진짜 가지가지 한다\", 0),\n","    (\"꺼져\", 0),\n","    (\"그렇게 살아서 뭘하겠니\", 0),\n","    # 재용님 - 주식이 파란불이다?\n","    (\"오늘 주식이 파란불이야\", 0),\n","    # 지현님\n","    (\"나 오늘 예민해\", 0),\n","    (\"주식이 떨어졌다\", 0),\n","    (\"콩나물 다시는 안먹어\", 0),\n","    (\"코인 시즌 끝났다\", 0),\n","    (\"배고파 죽을 것 같아\", 0),\n","    (\"한강 몇도냐\", 0),\n","    (\"집가고 싶다\", 0),\n","    (\"나 보기가 역겨워\", 0),  # 긍정적인 확률이 0\n","    # 진환님\n","    (\"잘도 그러겠다\", 0),\n","    (\"주말 언제\", 0)  # -> 주말 언제오는거야를 예측하기 위해선 이런 데이터도 필요하다.\n","]\n","\n","\n","TEST = [\n","    (\"행복한 쉬는날\", 1),\n","    (\"운수좋은 날\", 1),\n","    (\"도저히 못 해 먹겠다\", 0),\n","    (\"주말이 언제오는거야\", 0)\n","]\n","\n","# builders #\n","def build_X(sents: List[str], tokenizer: Tokenizer, max_length: int) -> torch.Tensor:\n","    seqs = tokenizer.texts_to_sequences(texts=sents)\n","    seqs = pad_sequences(sequences=seqs, padding=\"post\", maxlen=max_length, value=0)\n","    return torch.LongTensor(seqs)\n","\n","\n","def build_y(labels: List[int]) -> torch.Tensor:\n","    return torch.FloatTensor(labels)\n","\n","\n","class SimpleLSTM(torch.nn.Module):\n","    def __init__(self, vocab_size: int, hidden_size: int, embed_size: int):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.vocab_size = vocab_size\n","        self.embed_size = embed_size\n","        # 여기는 학습해야하는 가중치를 정의하는 부분.\n","        # 우리의 목표는, 이 가중치 행렬을 최적화 하는 것.\n","        # 이 가중치의 shape는 어떻게 결정?\n","        # 1. 일단 정의만 하고 스킵\n","        # 2. 데이터의 흐름을 차원을 트래킹하면서 확인\n","        # 3. (A, B) * (B, C) = (A, C)\n","        self.E = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n","        self.W = torch.nn.Linear(in_features=self.hidden_size + self.embed_size, out_features=self.hidden_size*4)\n","        self.W_hy = torch.nn.Linear(in_features=hidden_size, out_features=1)  # (H, 1)\n","        # 웨이트의 초기값을 더 적절하게 설정\n","        torch.nn.init.xavier_uniform_(self.E.weight)\n","        torch.nn.init.xavier_uniform_(self.W.weight)\n","        torch.nn.init.xavier_uniform_(self.W_hy.weight)\n","\n","    def forward(self, X: torch.Tensor):\n","        \"\"\"\n","        :param X: (N, L)\n","        :return:\n","        \"\"\"\n","        # 영성님 - torch.zeroes\n","        C_t = torch.zeros(size=(X.shape[0], self.hidden_size))  # (N, H)\n","        H_t = torch.zeros(size=(X.shape[0], self.hidden_size))  # (N, H)\n","        for time in range(X.shape[1]):\n","            X_t = X[:, time]  # (N, L) -> (N, 1)\n","            X_t = self.E(X_t)  # (N, 1) -> (N, E)\n","            H_cat_X = torch.cat([H_t, X_t], dim=1)  # (N, H), (N, E) -> (N, H+E)\n","            Gates = self.W(H_cat_X)  # (N, H+E) * (?=H+E, ?=H*4) -> (N, H * 4)\n","            I = torch.sigmoid(Gates[:, :self.hidden_size])# 입력게이트의 출력값.\n","            F = torch.sigmoid(Gates[:, self.hidden_size:self.hidden_size*2])  # 망각게이트의 출력값.\n","            O = torch.sigmoid(Gates[:, self.hidden_size*2:self.hidden_size*3]) # 출력게이트의 출력값.\n","            H_temp = torch.tanh(Gates[:, self.hidden_size*3:self.hidden_size*4])  # 임시 단기기억. (G)/\n","            # 이제 뭐해요?\n","            # 영성님 현재 시간대 장기기억 C_t를 만들어봅시다!\n","            # 뭘잊어야 하나? torch.mul(F, C_t)  (N, H) 성분곱 (N, H) -> (N, H)\n","            C_t = torch.mul(F, C_t) + torch.mul(I, H_temp)\n","            # O (N, H) -> 범위 = [0, 1]\n","            # 필요없는 기억에 해당하는 차원 = 0에 가깝게 알아서 조절\n","            # 필요한 기억의 차원 = 1에 가깝게 알아서 조절.\n","            H_t = torch.mul(O, torch.tanh(C_t))  # (N, H) 성분곱 (N,H) -> (N, H)\n","        return H_t\n","\n","    def predict(self, X: torch.Tensor) -> torch.Tensor:\n","        H_t = self.forward(X)  # (N, L) -> (N, H)\n","        y_pred = self.W_hy(H_t)  # (N, H) -> (N, 1)\n","        y_pred = torch.sigmoid(y_pred)  # (N, H) -> (N, 1)\n","        return y_pred\n","\n","    def training_step(self, X: torch.Tensor, y:torch.Tensor) -> torch.Tensor:\n","        # 배치가 들어왔을 때, 로스를 계산하면 된다.\n","        H_last = self.forward(X)  # (N, L) -> (N, H)\n","        # 이것으로부터, 긍정 / 부정일 확률을 구하고 싶다.\n","        # (N, H) * ???-(H, 1) -> (N, 1) - 어떻게 할 수 있을까?\n","        Hy = self.W_hy(H_last)  # (N, H) * (H, 1) -> (N, 1)\n","        # 그럼 Hy 의 범위가  [0, 1]?  아니다.\n","        # (N, 1)\n","        Hy_normalized = torch.sigmoid(Hy)  # [-inf, inf] -> [0, 1]\n","        # (N, 1), (N,) -> 하나로 통일하기 위해, y의 맞춘다.\n","        Hy_normalized = torch.reshape(Hy_normalized, y.shape)\n","        # print(Hy_normalized)\n","        # N개의 로스를 다 더해서, 이 배치의 로스를 구한다.\n","        loss = F.binary_cross_entropy(input=Hy_normalized, target=y).sum()\n","        return loss\n","\n","\n","\n","# hyper paraamters #\n","EPOCHS = 900\n","LR = 0.0001\n","HIDDEN_SIZE = 32\n","EMBED_SIZE = 12\n","MAX_LENGTH = 30\n","\n","\n","class Analyser:\n","\n","    def __init__(self, lstm: SimpleLSTM, tokenizer: Tokenizer, max_length: int):\n","        self.lstm = lstm\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __call__(self, text: str) -> float:\n","        X = build_X(sents=[text], tokenizer=self.tokenizer, max_length=self.max_length)\n","        y_pred = self.lstm.predict(X)\n","        return y_pred.item()\n","\n","\n","def main():\n","    # 문장들\n","    sents = [sent for sent, label in DATA]\n","    # 레이블들\n","    labels = [label for sent, label in DATA]\n","    tokenizer = Tokenizer(char_level=True)\n","    tokenizer.fit_on_texts(texts=sents)\n","    X: torch.Tensor = build_X(sents, tokenizer, MAX_LENGTH)  # (N, L)\n","    y: torch.Tensor = build_y(labels)  # (N, 1)\n","\n","    vocab_size = len(tokenizer.word_index.keys())\n","    vocab_size += 1\n","\n","    lstm = SimpleLSTM(vocab_size, HIDDEN_SIZE, EMBED_SIZE)\n","    optimizer = torch.optim.Adam(lstm.parameters(), lr=LR)\n","\n","    for epoch in range(EPOCHS):\n","        loss = lstm.training_step(X, y)  # loss 계산.\n","        loss.backward()  # 오차 역전파\n","        optimizer.step()  # 경사도 하강\n","        optimizer.zero_grad()  #  다음 에폭에서 기울기가 축적이 되지 않도록 리셋\n","        print(epoch, \"-->\", loss.item())\n","\n","    analyser = Analyser(lstm, tokenizer, MAX_LENGTH)\n","\n","    print(\"#### TRAIN TEST ####\")\n","    # traing accuracy\n","    correct = 0\n","    total = len(DATA)\n","    for sent, y in DATA:\n","        y_pred = analyser(sent)\n","        if y_pred >= 0.5 and y == 1:\n","            correct += 1\n","        elif y_pred < 0.5 and y == 0:\n","            correct += 1\n","    print(\"training acc:\", correct / total)\n","\n","\n","    print(\"#### TEST ####\")\n","    for sent, y in TEST:\n","        print(sent, y, analyser(sent))\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]}]}